{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strategic-apache",
   "metadata": {},
   "source": [
    "# Resilient RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-taxation",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-quick",
   "metadata": {},
   "source": [
    "So if in the last lesson, we saw how our RDDs are distributed, and that we distribute our dataset across a number of partitions, in this lesson we'll learn more about how RDDs achieve resiliency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-transport",
   "metadata": {},
   "source": [
    "### Building Fault Tolerance In Memory\n",
    "\n",
    "So as we know, the principal feature of Spark is that it provides for in memory storage.  Now in memory storage comes with some challenges -- mainly that even thouh we are not saving any updates to disk, we still want our spark cluster to be *fault tolerant*.  This means that even if one of our nodes goes down, we do not want the data on that node to be lost.\n",
    "\n",
    "Normally, distributed databases achieve this by copying partitions of the data to multiple nodes.  This way if one node goes down, there is still a backup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-trademark",
   "metadata": {},
   "source": [
    "<img src=\"./copied_data.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-uncertainty",
   "metadata": {},
   "source": [
    "The cost to this, however, (1) this copying takes up a significant amount of space, and (2) that it requires copying data over a the cluster's network, whose bandwidth is lower than the RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-durham",
   "metadata": {},
   "source": [
    "> Below, our data is moved from one node to another.  With a lot of data, and narrow bandwidth, this can be a slow process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-karaoke",
   "metadata": {},
   "source": [
    "<img src=\"./network_slow.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-precipitation",
   "metadata": {},
   "source": [
    "In Spark things are done differently.  Instead of copying the data over, from one node to another, Spark instead keeps track of all of the steps to recreate our dataset.  So if the node goes down, it can simply reapply those steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-obligation",
   "metadata": {},
   "source": [
    "We'll learn more about this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-aaron",
   "metadata": {},
   "source": [
    "### DAGs in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-vector",
   "metadata": {},
   "source": [
    "Let's get started by again, connecting to our cluster, and then creating an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tight-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stock-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brief-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-actor",
   "metadata": {},
   "source": [
    "This time we'll change our data.  We can do so by using the `map` function to capitalize each word in our list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "every-punishment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-great",
   "metadata": {},
   "source": [
    "> So above, `movie` represents each title in the list, and we call `title` on each one to capitalize.  We see the results of the transformation with `collect`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-private",
   "metadata": {},
   "source": [
    "Now let's see what happens under the hood.  As we said above, to keep our data fault tolerant, instead of copying data from one node to another, Spark instead keeps track of all of the steps to recreate the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-david",
   "metadata": {},
   "source": [
    "<img src=\"./rdd_one_to_two.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-hobby",
   "metadata": {},
   "source": [
    "So what the diagram above shows, is that when we executed the line `rdd.map(lambda movie: movie.title()).collect()`, we actually created a second RDD.  And spark has logged the step needed to go from one RDD to the second one.  So if the node that has the capitalized `Avatar` goes down, we can just reapply the `map(lambda movie: movie.title())` function to recreate the data.\n",
    "\n",
    "So there are two takeaways from the above.  The first is that when we apply a transformation to a dataset, we are actually creating a new RDD.  This is because RDD's are read only, so if apply a transformation, we'll create a new RDD in the process.  The second takeaway, is that Spark stores the steps to go from the first rdd to the second one, and reapplies those steps to the relevant data if a node goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-eugene",
   "metadata": {},
   "source": [
    "### Only Coarse Transformations\n",
    "\n",
    "Because keeping track of every tiny change that happens to a dataset, Spark does not give us the functions to apply changes just to specific records.  Rather when we apply changes, we must apply these changes to the entire dataset.  For example, above, we capitalized every record with the `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rapid-juvenile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-buffalo",
   "metadata": {},
   "source": [
    "Or, with our RDDs, we can also go through every record, and only select those that begin with the letter `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "northern-lover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark knight', 'dunkirk']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda movie: movie[0] == 'd').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-elevation",
   "metadata": {},
   "source": [
    "And of course we can chain our two methods together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "selected-eating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda movie: movie.title()).filter(lambda movie: movie[0] == 'd' or movie[0] == 'D').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-enterprise",
   "metadata": {},
   "source": [
    "> So above we first capitalized each of our movies, and then selected those that begin with a capital `D`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-leisure",
   "metadata": {},
   "source": [
    "The point from above, though is that whether we use `map` or `filter`, each step applies to *every* record.  These types of transformations are called `coarse grained transformations` - and these are the only kinds of transformations that Spark allows.  If we were to select individual records and then make changes to them, this would be fine-grained transformations.  By only allowing coarse grained transformations, this makes it easier on Spark to log the changes made from one RDD to another -- as only these coarse grained transformations are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-italic",
   "metadata": {},
   "source": [
    "<img src=\"./filter_map.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-albania",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-morocco",
   "metadata": {},
   "source": [
    "So we saw in this lesson that Spark achieves fault tolerance by keeping a recording of the transformations needed to recreate our data.  Because the RDDs are read only, so when we transform our data, really we are creating a new RDD.  And Spark keeps track of the steps necessary to go from one transformation to the other.\n",
    "\n",
    "<img src=\"./rdd_one_to_two.jpg\" width=\"40%\">\n",
    "\n",
    "To make recording these steps easier, on Spark RDDs, we can only apply coarse grained transformations, which apply to the entire Spark dataset.  We'll learn more of these transformations in the following lesson, but to start, `map` which applies the same change to every record, and `filter` which selects from a set of elements are two coarse grained transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-algorithm",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Presenting RDDs](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "\n",
    "[RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-saturday",
   "metadata": {},
   "source": [
    "* [RDDs Simplified](https://vishnuviswanath.com/spark_rdd)\n",
    "\n",
    "* [Databricks RDDs](https://databricks.com/glossary/what-is-rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-valve",
   "metadata": {},
   "source": [
    "[Databricks best practices](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
