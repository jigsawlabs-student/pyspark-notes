{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "miniature-founder",
   "metadata": {},
   "source": [
    "# Parallel Processing Keystones\n",
    "\n",
    "### Map Reduce and Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-merit",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stainless-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-smell",
   "metadata": {},
   "source": [
    "### Map Reduce in Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-editing",
   "metadata": {},
   "source": [
    "Now from here, we'll create our list of movies in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "normal-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Shazam!', 'Minari', 'Captain Marvel', \n",
    "          'Pulp Fiction', 'Casablanca', 'Michael Clayton',\n",
    "          'Sicario']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-beverage",
   "metadata": {},
   "source": [
    "And from there turn it into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "powered-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Shazam!', 'Minari', 'Captain Marvel', \n",
    "          'Pulp Fiction', 'Casablanca', 'Michael Clayton',\n",
    "          'Sicario']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "absent-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "colored-guarantee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael Clayton']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda movie: movie == 'Michael Clayton').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surface-people",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-recovery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "animal-player",
   "metadata": {},
   "source": [
    "*  if we say use a `filter` to look for the movie 'Michael Clayton' across the entire dataset, we perform this filter task four times across the cluster in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-female",
   "metadata": {},
   "source": [
    "<img src=\"./map_red_cluster.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accredited-relay",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-129544ec07ce>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-129544ec07ce>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    count all of the records that begin with M\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "count all of the records group by first letter\n",
    "mapping stage\n",
    "2\n",
    "3\n",
    "\n",
    "2\n",
    "3\n",
    "\n",
    "reduce \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aware-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_name = 'Michael Clayton'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-remove",
   "metadata": {},
   "source": [
    "### Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-arbitration",
   "metadata": {},
   "source": [
    "* Shuffling occurs when an operation requires us to send our data across partitions to successfully perform a query.  \n",
    "\n",
    "> Why we care: Sending data across nodes is often a time intensive.\n",
    "\n",
    "> <img src=\"./network_slow.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "count all of the records that begin with M\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-remainder",
   "metadata": {},
   "source": [
    "#### An example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-enhancement",
   "metadata": {},
   "source": [
    "Say we have the following movies across the following partitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heard-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Shazam!', 'Minari', 'Captain Marvel', \n",
    "          'Pulp Fiction', 'Casablanca', 'Michael Clayton',\n",
    "          'Sicario']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "falling-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rdd = sc.parallelize(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-principal",
   "metadata": {},
   "source": [
    "* Partition 1\n",
    "    * Shazam!\n",
    "    * Minari\n",
    "* Partition 2\n",
    "    * Captain Marvel\n",
    "    * Pulp Fiction\n",
    "* Partition 3\n",
    "    * Casablanca\n",
    "    * Michael Clayton\n",
    "* Partition 4\n",
    "    * Sicario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "swedish-equation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', ['Shazam!', 'Sicario']),\n",
       " ('C', ['Captain Marvel', 'Casablanca']),\n",
       " ('M', ['Minari', 'Michael Clayton']),\n",
       " ('P', ['Pulp Fiction'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.groupBy(lambda movie: movie[0]).map(lambda group: (group[0],  list(group[1]) )).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-warren",
   "metadata": {},
   "source": [
    "* Group the movies together by their first letter.  \n",
    "\n",
    "This grouping will require sending some movies from one worker node to another so that they can reside together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "distinguished-father",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', ['Minari', 'Michael Clayton']),\n",
       " ('S', ['Shazam!', 'Sicario']),\n",
       " ('C', ['Captain Marvel', 'Casablanca']),\n",
       " ('P', ['Pulp Fiction'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupBy(lambda movie: movie[0]).map(lambda group: (group[0], list(group[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "challenging-universal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', ['Minari', 'Michael Clayton']),\n",
       " ('S', ['Shazam!', 'Sicario']),\n",
       " ('C', ['Captain Marvel', 'Casablanca']),\n",
       " ('P', ['Pulp Fiction'])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rural-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.122:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>films</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=films>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-basement",
   "metadata": {},
   "source": [
    "### Looking under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-darkness",
   "metadata": {},
   "source": [
    "A great way to understand Pyspark is with the spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "wired-national",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jeffreys-air.home:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>films</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=films>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-facility",
   "metadata": {},
   "source": [
    "And then if we click on the Spark UI link, we'll see something like the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-korean",
   "metadata": {},
   "source": [
    "> <img src=\"./completed_jobs.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-heavy",
   "metadata": {},
   "source": [
    "* Understanding Map partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-raise",
   "metadata": {},
   "source": [
    "* map\n",
    "    * will execute our code one time for each record.  \n",
    "        * So 2,000 records our related code will be called 2000 times.  \n",
    "* map partitions\n",
    "    * function is called only once per partition.  \n",
    "    * so if those 2,000 records are divided into 20 partitions, then the related function is only called 20 times.  This is more efficient call.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "several-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-lambda",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Shuffling Documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations)\n",
    "\n",
    "[5 hour pyspark tutorial](https://www.youtube.com/watch?v=GFC2gOL1p9k&t=2743s)\n",
    "\n",
    "[Map vs Map Partition](https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/)\n",
    "\n",
    "[Group by key](https://backtobazics.com/big-data/spark/apache-spark-groupbykey-example/)\n",
    "\n",
    "[Pyspark Google Colab](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
