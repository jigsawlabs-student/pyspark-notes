{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collaborative-mentor",
   "metadata": {},
   "source": [
    "# Parallel Processing Keystones\n",
    "\n",
    "### Map Reduce and Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-nashville",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "induced-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-hindu",
   "metadata": {},
   "source": [
    "### Map Reduce in Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-night",
   "metadata": {},
   "source": [
    "Now from here, we'll create our list of movies in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cosmetic-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Shazam!', 'Minari', 'Captain Marvel', \n",
    "          'Pulp Fiction', 'Casablanca', 'Michael Clayton',\n",
    "          'Sicario']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-basin",
   "metadata": {},
   "source": [
    "And from there turn it into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "strategic-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Shazam!', 'Minari', 'Captain Marvel', \n",
    "          'Pulp Fiction', 'Casablanca', 'Michael Clayton',\n",
    "          'Sicario']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "formed-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "framed-punch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael Clayton']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda movie: movie == 'Michael Clayton').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "english-aerospace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-group",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "guided-sunrise",
   "metadata": {},
   "source": [
    "*  if we say use a `filter` to look for the movie 'Michael Clayton' across the entire dataset, we perform this filter task four times across the cluster in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-small",
   "metadata": {},
   "source": [
    "<img src=\"./map_red_cluster.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automotive-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_name = 'Michael Clayton'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-message",
   "metadata": {},
   "source": [
    "### Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-internship",
   "metadata": {},
   "source": [
    "* Shuffling occurs when an operation requires us to send our data across partitions to successfully perform a query.  \n",
    "\n",
    "> Why we care: Sending data across nodes is often a time intensive.\n",
    "\n",
    "> <img src=\"./network_slow.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-holder",
   "metadata": {},
   "source": [
    "#### An example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-assignment",
   "metadata": {},
   "source": [
    "Say we have the following movies across the following partitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-alignment",
   "metadata": {},
   "source": [
    "* Partition 1\n",
    "    * Shazam!\n",
    "    * Minari\n",
    "* Partition 2\n",
    "    * Captain Marvel\n",
    "    * Pulp Fiction\n",
    "* Partition 3\n",
    "    * Casablanca\n",
    "    * Michael Clayton\n",
    "* Partition 4\n",
    "    * Sicario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-straight",
   "metadata": {},
   "source": [
    "* Group the movies together by their first letter.  \n",
    "\n",
    "This grouping will require sending some movies from one worker node to another so that they can reside together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inappropriate-military",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', ['Minari', 'Michael Clayton']),\n",
       " ('S', ['Shazam!', 'Sicario']),\n",
       " ('C', ['Captain Marvel', 'Casablanca']),\n",
       " ('P', ['Pulp Fiction'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupBy(lambda movie: movie[0]).map(lambda group: (group[0], list(group[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "wrapped-import",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', ['Minari', 'Michael Clayton']),\n",
       " ('S', ['Shazam!', 'Sicario']),\n",
       " ('C', ['Captain Marvel', 'Casablanca']),\n",
       " ('P', ['Pulp Fiction'])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-lewis",
   "metadata": {},
   "source": [
    "### Looking under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-batman",
   "metadata": {},
   "source": [
    "A great way to understand Pyspark is with the spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "friendly-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jeffreys-air.home:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>films</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=films>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-access",
   "metadata": {},
   "source": [
    "And then if we click on the Spark UI link, we'll see something like the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-static",
   "metadata": {},
   "source": [
    "> <img src=\"./completed_jobs.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-employment",
   "metadata": {},
   "source": [
    "* Understanding Map partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-substance",
   "metadata": {},
   "source": [
    "* map\n",
    "    * will execute our code one time for each record.  \n",
    "        * So 2,000 records our related code will be called 2000 times.  \n",
    "* map partitions\n",
    "    * function is called only once per partition.  \n",
    "    * so if those 2,000 records are divided into 20 partitions, then the related function is only called 20 times.  This is more efficient call.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "golden-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-millennium",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Shuffling Documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations)\n",
    "\n",
    "[5 hour pyspark tutorial](https://www.youtube.com/watch?v=GFC2gOL1p9k&t=2743s)\n",
    "\n",
    "[Map vs Map Partition](https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/)\n",
    "\n",
    "[Group by key](https://backtobazics.com/big-data/spark/apache-spark-groupbykey-example/)\n",
    "\n",
    "[Pyspark Google Colab](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
