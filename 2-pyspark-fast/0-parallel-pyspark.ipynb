{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legal-crazy",
   "metadata": {},
   "source": [
    "# Pyspark - Parallel Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-priority",
   "metadata": {},
   "source": [
    "* Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-distance",
   "metadata": {},
   "source": [
    "When working with just very large datasets in memory, like say in pandas, performing a simple operation like just looking finding records in a certain date, or sorting the data can take upwards of 30 minutes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-victorian",
   "metadata": {},
   "source": [
    "* Spark Solution: Split up data and operate in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-facial",
   "metadata": {},
   "source": [
    "> * **driver node**\n",
    "\n",
    "> * **worker nodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-compression",
   "metadata": {},
   "source": [
    "<img src=\"./spark_cluster_disk.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-india",
   "metadata": {},
   "source": [
    "### Looking at an Executor: CPU Cores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-window",
   "metadata": {},
   "source": [
    "As we know, when we read our data into Spark, we can store that data distributed through worker nodes located in the cluster.  \n",
    "\n",
    "> *the software* operating on each worker node is referred to as an **executor**.  There is just one executor per worker node, and for this reason, instead of worker nodes, will refer to executors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-jewelry",
   "metadata": {},
   "source": [
    "> <img src=\"./cluster_executor.jpg\" width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-aviation",
   "metadata": {},
   "source": [
    "But beyond executors, each executor may have multiple CPUs, and each CPU may have one or more cores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-enough",
   "metadata": {},
   "source": [
    "* what constrains our ability to partition in our data is the **number of cores** across all of the executors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU - \n",
    "    1 cpu\n",
    "    4 cores\n",
    "    4 tasks in parallel  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-argument",
   "metadata": {},
   "source": [
    "Remember -- why do we care?  Parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-peace",
   "metadata": {},
   "source": [
    "<img src=\"./executor_closer.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-absorption",
   "metadata": {},
   "source": [
    "This entire dataset, distributed across our various nodes and partitioned per each core is called a **resilient distributed dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-billion",
   "metadata": {},
   "source": [
    "### See it in action: Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-thong",
   "metadata": {},
   "source": [
    "Before we can get to our executors, the first step we need to perform is create our Spark Context.   Our Spark context, is how we interact with our driver, which is our entrypoint into our Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detected-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "substantial-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "elder-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-listing",
   "metadata": {},
   "source": [
    "> <img src=\"./cluster_executor.jpg\" width='90%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "personal-chinese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(movies)\n",
    "\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "trying-success",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain Marvel']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda movie: movie == 'Captain Marvel').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-pendant",
   "metadata": {},
   "source": [
    "### Seeing Parallelization in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-quantum",
   "metadata": {},
   "source": [
    "We can get a sense of this parallelization if we pass our data into Spark.  In fact one way to feed our data into spark is with a method called parallelize. \n",
    "\n",
    "> For example, we can start with a list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "democratic-century",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "previous-closer",
   "metadata": {},
   "source": [
    "And from there, we move this data into Spark with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "twenty-discrimination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd = sc.parallelize(movies)\n",
    "\n",
    "movies_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-conjunction",
   "metadata": {},
   "source": [
    "> And then we can look at the number of cores in that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quality-train",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "superb-capture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain Marvel']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.filter(lambda movie: movie == 'Captain Marvel').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-fancy",
   "metadata": {},
   "source": [
    "> So in the line above, our data was first split up four ways, and then we looked for Captain Marvel on each slice of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-message",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* [Spark Internals Gitbook](https://books.japila.pl/apache-spark-internals/overview/)\n",
    "\n",
    "* [Drivers and Executors Knoldus Blog](https://blog.knoldus.com/understanding-the-working-of-spark-driver-and-executor/)\n",
    "\n",
    "* [Drivers and Executors StackOverflow](https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster)\n",
    "\n",
    "* [Presenting RDDs Berkeley Paper](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "\n",
    "* [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "* [RDDs Simplified](https://vishnuviswanath.com/spark_rdd)\n",
    "\n",
    "* [Databricks RDDs](https://databricks.com/glossary/what-is-rdd)\n",
    "\n",
    "* [Databricks best practices gitbook](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
