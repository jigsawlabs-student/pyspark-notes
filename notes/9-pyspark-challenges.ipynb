{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classified-latino",
   "metadata": {},
   "source": [
    "# Working with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-vegetation",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-trigger",
   "metadata": {},
   "source": [
    "As we know, Spark allows us to store data in memory in a distributed cluster of nodes.  In that cluster we have the driver node, as well as compute nodes, or executors that carry out tasks on the various partitions of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-generation",
   "metadata": {},
   "source": [
    "<img src=\"./spark_cluster_partition.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-stock",
   "metadata": {},
   "source": [
    "We can see that the diagram above the SparkContext is our way of controlling the driver, and then we can use that Spark context to read in a data set like our table of movies above.  That table of movies is *distributed* across the nodes -- a distributed dataset if you will, and because Spark designs the dataset to be recoverable when a given node fails, Spark calls this dataset a Resilient Distributed Dataset, or RDD.  \n",
    "\n",
    "> So above, the table of movies, distributed across the various nodes is an RDD.\n",
    "\n",
    "In this lesson, we'll learn how to create our SparkContext -- the entrypoint to our cluster -- as well as our distributed dataset the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-involvement",
   "metadata": {},
   "source": [
    "### Building our Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-bangkok",
   "metadata": {},
   "source": [
    "So as we saw above, our Spark Context is our way of connecting with our Spark cluster.  And we create our Spark Context with the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-label",
   "metadata": {},
   "source": [
    "> First, we install the python Pyspark package with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sapphire-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-parameter",
   "metadata": {},
   "source": [
    "Now it's time to create the Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "electrical-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "compatible-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-terminal",
   "metadata": {},
   "source": [
    "So above, we create the SparkContext by specifying the information needed to connect to our spark cluster.  Let's walk through the steps to doing so.\n",
    "\n",
    "First, we set up some configuration for the spark session.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "small-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-mouth",
   "metadata": {},
   "source": [
    "Now above, we call `SparkConf().setAppName(\"films\")` to set up our application name, which is just a name that we specify.  And then we use the `setMaster(\"local[2]\")` method to specify the location of the cluster we want to connect to.  Generally, we'll pass in the url to specify this location, but above we are connecting to the cluster on our local computer so we just use the string `\"local\"`.  The square brackets are there to indicate the number of parallel proccesses we want running.  \n",
    "\n",
    "> So above we are specifying to processes running on two cores of our computer.  This means that two processes can be running in parallel on our spark cluster.  If we want to use as many cores as are available on our computer we can use `\"local[*]\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-copper",
   "metadata": {},
   "source": [
    "Now that we've created the configuration for our SparkContext, in the second line above, we call the `SparkContext.getOrCreate` method to create a new context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "composed-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-injury",
   "metadata": {},
   "source": [
    "That's it, our SparkContext is all set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "wooden-vaccine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jeffreys-air.home:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>films</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=films>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-creator",
   "metadata": {},
   "source": [
    "### Creating an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-philosophy",
   "metadata": {},
   "source": [
    "Once we have created our SparkContext, we can now read in our data from an external resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-malpractice",
   "metadata": {},
   "source": [
    "<img src=\"./spark_cluster_partition.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-cowboy",
   "metadata": {},
   "source": [
    "Now that we have created our context, we can use our context to create an resilient distributed dataset.  Now an RDD can be a really any data that can be split up among multiple processes.  To start, let's define some sample data, which can just be a list of dictionaries representing four movies below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "caroline-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = [{'id': '287947',\n",
    "  'title': 'Shazam!',\n",
    "  'poster': 'https://image.tmdb.org/t/p/w500/xnopI5Xtky18MPhK40cZAGAOVeV.jpg',\n",
    "  'overview': 'A boy is given the ability to become an adult superhero in times of need with a single magic word.',\n",
    "  'release_date': 1553299200,\n",
    "  'genres': ['Action', 'Comedy', 'Fantasy']},\n",
    " {'id': '299537',\n",
    "  'title': 'Captain Marvel',\n",
    "  'poster': 'https://image.tmdb.org/t/p/w500/AtsgWhDnHTq68L0lLsUrCnM7TjG.jpg',\n",
    "  'overview': 'The story follows Carol Danvers as she becomes one of the universe’s most powerful heroes when Earth is caught in the middle of a galactic war between two alien races. Set in the 1990s, Captain Marvel is an all-new adventure from a previously unseen period in the history of the Marvel Cinematic Universe.',\n",
    "  'release_date': 1551830400,\n",
    "  'genres': ['Action', 'Adventure', 'Science Fiction']},\n",
    " {'id': '522681',\n",
    "  'title': 'Escape Room',\n",
    "  'poster': 'https://image.tmdb.org/t/p/w500/8Ls1tZ6qjGzfGHjBB7ihOnf7f0b.jpg',\n",
    "  'overview': 'Six strangers find themselves in circumstances beyond their control, and must use their wits to survive.',\n",
    "  'release_date': 1546473600,\n",
    "  'genres': ['Thriller', 'Action', 'Horror', 'Science Fiction']},\n",
    " {'id': '166428',\n",
    "  'title': 'How to Train Your Dragon: The Hidden World',\n",
    "  'poster': 'https://image.tmdb.org/t/p/w500/xvx4Yhf0DVH8G4LzNISpMfFBDy2.jpg',\n",
    "  'overview': 'As Hiccup fulfills his dream of creating a peaceful dragon utopia, Toothless’ discovery of an untamed, elusive mate draws the Night Fury away. When danger mounts at home and Hiccup’s reign as village chief is tested, both dragon and rider must make impossible decisions to save their kind.',\n",
    "  'release_date': 1546473600,\n",
    "  'genres': ['Animation', 'Family', 'Adventure']}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-scheme",
   "metadata": {},
   "source": [
    "Now we start with this data as a simple list of dictionaries in Python, but we can convert it into an RDD with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "basic-partition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[15] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd = sc.parallelize(movies)\n",
    "movies_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "hairy-bikini",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-saturday",
   "metadata": {},
   "source": [
    "And we can attempt to parallelize this across multiple four processes by adding a second argument where we specify the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "integrated-riding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[17] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd = sc.parallelize(movies, 4)\n",
    "movies_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-contribution",
   "metadata": {},
   "source": [
    "> We can see if this did separate the data into four partitions with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "victorian-lewis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-party",
   "metadata": {},
   "source": [
    "The importance of this is now we can perform operations across four different partitions of the dataset simultanously.  So if we want to find a matching film record, we can search across four different partitions simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-luxury",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-delay",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how RDDs are distributed datasets.  We also saw how we connect to the driver of our cluster, by creating a SparkContext with something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deadly-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-vermont",
   "metadata": {},
   "source": [
    "By specifying `setMaster(\"local[2]\")`, above we said specified to distribute our tasks across two different cores.  And now with our SparkContext set up, we can turn a collection in Python into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "related-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Shazam!', 'Captain Marvel', 'Escape Room', 'How to Train Your Dragon: The Hidden World']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fantastic-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rdd = sc.parallelize(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-hunger",
   "metadata": {},
   "source": [
    "This is the first attribute of an RDD, our data is now partitioned, and when we eventually operate on this data, we can operate on different components of it simultanously.  We can see this with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "becoming-independence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-nickname",
   "metadata": {},
   "source": [
    "# Resilient RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-trademark",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-stake",
   "metadata": {},
   "source": [
    "So if in the last lesson, we saw how our RDDs are distributed, and that we distribute our dataset across a number of partitions, in this lesson we'll learn more about how RDDs achieve resiliency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-diameter",
   "metadata": {},
   "source": [
    "### Building Fault Tolerance In Memory\n",
    "\n",
    "So as we know, the principal feature of Spark is that it provides for in memory storage.  Now in memory storage comes with some challenges -- mainly that even thouh we are not saving any updates to disk, we still want our spark cluster to be *fault tolerant*.  This means that even if one of our nodes goes down, we do not want the data on that node to be lost.\n",
    "\n",
    "Normally, distributed databases achieve this by copying partitions of the data to multiple nodes.  This way if one node goes down, there is still a backup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-headline",
   "metadata": {},
   "source": [
    "<img src=\"./copied_data.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-james",
   "metadata": {},
   "source": [
    "The cost to this, however, (1) this copying takes up a significant amount of space, and (2) that it requires copying data over a the cluster's network, whose bandwidth is lower than the RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-annual",
   "metadata": {},
   "source": [
    "> Below, our data is moved from one node to another.  With a lot of data, and narrow bandwidth, this can be a slow process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-october",
   "metadata": {},
   "source": [
    "<img src=\"./network_slow.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-dictionary",
   "metadata": {},
   "source": [
    "In Spark things are done differently.  Instead of copying the data over, from one node to another, Spark instead keeps track of all of the steps to recreate our dataset.  So if the node goes down, it can simply reapply those steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-textbook",
   "metadata": {},
   "source": [
    "We'll learn more about this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-newcastle",
   "metadata": {},
   "source": [
    "### DAGs in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-reason",
   "metadata": {},
   "source": [
    "Let's get started by again, connecting to our cluster, and then creating an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "frozen-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "posted-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ethical-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-scholar",
   "metadata": {},
   "source": [
    "This time we'll change our data.  We can do so by using the `map` function to capitalize each word in our list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "complex-input",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-dominican",
   "metadata": {},
   "source": [
    "> So above, `movie` represents each title in the list, and we call `title` on each one to capitalize.  We see the results of the transformation with `collect`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-acceptance",
   "metadata": {},
   "source": [
    "Now let's see what happens under the hood.  As we said above, to keep our data fault tolerant, instead of copying data from one node to another, Spark instead keeps track of all of the steps to recreate the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-ferry",
   "metadata": {},
   "source": [
    "<img src=\"./rdd_one_to_two.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-turtle",
   "metadata": {},
   "source": [
    "So what the diagram above shows, is that when we executed the line `rdd.map(lambda movie: movie.title()).collect()`, we actually created a second RDD.  And spark has logged the step needed to go from one RDD to the second one.  So if the node that has the capitalized `Avatar` goes down, we can just reapply the `map(lambda movie: movie.title())` function to recreate the data.\n",
    "\n",
    "So there are two takeaways from the above.  The first is that when we apply a transformation to a dataset, we are actually creating a new RDD.  This is because RDD's are read only, so if apply a transformation, we'll create a new RDD in the process.  The second takeaway, is that Spark stores the steps to go from the first rdd to the second one, and reapplies those steps to the relevant data if a node goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-roommate",
   "metadata": {},
   "source": [
    "### Only Coarse Transformations\n",
    "\n",
    "Because keeping track of every tiny change that happens to a dataset, Spark does not give us the functions to apply changes just to specific records.  Rather when we apply changes, we must apply these changes to the entire dataset.  For example, above, we capitalized every record with the `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "known-application",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-invasion",
   "metadata": {},
   "source": [
    "Or, with our RDDs, we can also go through every record, and only select those that begin with the letter `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "distant-ranch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark knight', 'dunkirk']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda movie: movie[0] == 'd').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-princess",
   "metadata": {},
   "source": [
    "And of course we can chain our two methods together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "pleased-macro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda movie: movie.title()).filter(lambda movie: movie[0] == 'd' or movie[0] == 'D').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-gossip",
   "metadata": {},
   "source": [
    "> So above we first capitalized each of our movies, and then selected those that begin with a capital `D`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-mistress",
   "metadata": {},
   "source": [
    "The point from above, though is that whether we use `map` or `filter`, each step applies to *every* record.  These types of transformations are called `coarse grained transformations` - and these are the only kinds of transformations that Spark allows.  If we were to select individual records and then make changes to them, this would be fine-grained transformations.  By only allowing coarse grained transformations, this makes it easier on Spark to log the changes made from one RDD to another -- as only these coarse grained transformations are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-cologne",
   "metadata": {},
   "source": [
    "<img src=\"./filter_map.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-paris",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-reduction",
   "metadata": {},
   "source": [
    "So we saw in this lesson that Spark achieves fault tolerance by keeping a recording of the transformations needed to recreate our data.  Because the RDDs are read only, so when we transform our data, really we are creating a new RDD.  And Spark keeps track of the steps necessary to go from one transformation to the other.\n",
    "\n",
    "<img src=\"./rdd_one_to_two.jpg\" width=\"40%\">\n",
    "\n",
    "To make recording these steps easier, on Spark RDDs, we can only apply coarse grained transformations, which apply to the entire Spark dataset.  We'll learn more of these transformations in the following lesson, but to start, `map` which applies the same change to every record, and `filter` which selects from a set of elements are two coarse grained transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-tanzania",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Presenting RDDs](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "\n",
    "[RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-antarctica",
   "metadata": {},
   "source": [
    "* [RDDs Simplified](https://vishnuviswanath.com/spark_rdd)\n",
    "\n",
    "* [Databricks RDDs](https://databricks.com/glossary/what-is-rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-anthropology",
   "metadata": {},
   "source": [
    "[Databricks best practices](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
