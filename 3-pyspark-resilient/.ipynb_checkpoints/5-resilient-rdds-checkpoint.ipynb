{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "twelve-closer",
   "metadata": {},
   "source": [
    "# Resilient RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-demand",
   "metadata": {},
   "source": [
    "### Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-receipt",
   "metadata": {},
   "source": [
    "We saw the two of the main components of Spark.  \n",
    "\n",
    "1. Spark primarily saves data to memory, and\n",
    "2. Spark stores a dataset distributed across executors, that query and operate on that data in parallel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-sunday",
   "metadata": {},
   "source": [
    "> <img src=\"./cluster_executor.jpg\" width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-prerequisite",
   "metadata": {},
   "source": [
    "* how does Spark does recover when data when a node goes down.  \n",
    "\n",
    "The *resilient* component of our Resilient Distributed Datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-heritage",
   "metadata": {},
   "source": [
    "### Fault Tolerance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-forty",
   "metadata": {},
   "source": [
    "> we still want our spark cluster to be *fault tolerant*.  \n",
    "    \n",
    "* Fault tolerant: Even if one of our nodes goes down, we do not want the data on that node to be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-going",
   "metadata": {},
   "source": [
    "### The normal approach: copy the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-wagner",
   "metadata": {},
   "source": [
    "> Normally, distributed databases achieve this by copying partitions of the data to multiple nodes.  This way if one node goes down, there is still a backup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-reducing",
   "metadata": {},
   "source": [
    "<img src=\"./copied_data.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-gilbert",
   "metadata": {},
   "source": [
    "> In the diagram below we can see that the `D` movies are copied to two different nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-dakota",
   "metadata": {},
   "source": [
    "Problems with copying:\n",
    "\n",
    "1. This copying takes up a significant amount of space, and \n",
    "2. It requires copying data over a the cluster's network, and oftentimes there may be narrow bandwidth to do so\n",
    "\n",
    "> The diagram below shows the copying process from one node to the other.  With a lot of data, and narrow bandwidth, this can be a slow process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-output",
   "metadata": {},
   "source": [
    "> <img src=\"./network_slow.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-occurrence",
   "metadata": {},
   "source": [
    "### Spark's Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-external",
   "metadata": {},
   "source": [
    "Instead of copying the data over, from one node to another, Spark instead keeps track of all of the steps to recreate our dataset in the driver node.\n",
    "\n",
    "* if the node goes down, it can simply reapply those steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-probe",
   "metadata": {},
   "source": [
    "### The Consequence: \n",
    "\n",
    "1. Only Coarse Transformations\n",
    "\n",
    "Because keeping track of every tiny change that happens to a dataset takes some work, Spark limits the kinds of transformations we can apply.  \n",
    "\n",
    "* When we apply changes, we must apply these changes to the *entire* RDD.  For example, above, we capitalized every record with the `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "asian-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['dunkirk', 'minari']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.filter(lambda movie: movie == 'dunkirk').map(lambda movie: movie.capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-deviation",
   "metadata": {},
   "source": [
    "2. Only Read Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-gospel",
   "metadata": {},
   "source": [
    "* when we apply a transformation to a dataset, we are actually creating a new RDD.  Again, we can see that in the DAG.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-disorder",
   "metadata": {},
   "source": [
    "\n",
    "So we are never updating an RDD.  \n",
    "\n",
    "> Our RDDs are **read only**, and whenever we filter or map through a dataset, we are creating a new RDD in the process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "apart-determination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-queensland",
   "metadata": {},
   "source": [
    "Or, with our RDDs, we can also go through every record, and only select those that begin with the letter `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "norman-graham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark knight', 'dunkirk']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda movie: movie[0] == 'd').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-absence",
   "metadata": {},
   "source": [
    "> But this is still considered an operation on the entire dataset because we search through every record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-topic",
   "metadata": {},
   "source": [
    "whether we use `map` or `filter`, each step applies to *every* record.  These types of transformations are called **coarse grained transformations** - and these are the only kinds of transformations that Spark allows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "least-extra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda movie: movie[0] == 'd').map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-rating",
   "metadata": {},
   "source": [
    "<img src=\"./filter_map.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-delicious",
   "metadata": {},
   "source": [
    "### Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-cologne",
   "metadata": {},
   "source": [
    "* Spark's approach for fault tolerance\n",
    "    * Keep record of transformations needed to recreate data\n",
    "        * Only coarse grained transformations (filter, map)\n",
    "        * And RDDs are read only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-desperate",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Spark Debugging Minibook](https://cs.famaf.unc.edu.ar/~damian/tmp/bib/Mini%20eBook%20-%20Apache%20Spark%20Monitoring%20and%20Debugging.pdf)\n",
    "\n",
    "[Presenting RDDs](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "\n",
    "[RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-template",
   "metadata": {},
   "source": [
    "* [RDDs Simplified](https://vishnuviswanath.com/spark_rdd)\n",
    "\n",
    "* [Databricks RDDs](https://databricks.com/glossary/what-is-rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-orleans",
   "metadata": {},
   "source": [
    "[Databricks best practices](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
