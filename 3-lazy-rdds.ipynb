{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "irish-devices",
   "metadata": {},
   "source": [
    "# Lazy RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-meter",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-cooking",
   "metadata": {},
   "source": [
    "So we saw in the last lesson that Spark achieves fault tolerance by keeping a recording of the transformations needed to recreate our data.\n",
    "\n",
    "<img src=\"./filter_map.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-ebony",
   "metadata": {},
   "source": [
    "These steps are an example of a directed acyclic graph, because we go from one step to another to arrive at the resulting RDD.  It turns out that recording the steps needed to transform the data is useful not just for fault tolerance but because it allows Spark to determine efficient ways to perform the prescribed steps.  We'll learn more about how spark achieves this efficiency in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-recommendation",
   "metadata": {},
   "source": [
    "### Looking Under the Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-portable",
   "metadata": {},
   "source": [
    "Let's again connect to our spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mineral-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"films\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-dubai",
   "metadata": {},
   "source": [
    "And then, let's again create an RDD from our movie records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ahead-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improving-initial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd = sc.parallelize(movies)\n",
    "movies_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-editing",
   "metadata": {},
   "source": [
    "And then let's capitalize the movies, and select the movies that begin with `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "civilian-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.map(lambda movie: movie.title()).filter(lambda movie: movie[0] == 'd' or movie[0] == 'D').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-tobacco",
   "metadata": {},
   "source": [
    "Now, as we wrote the code above, Spark should first capitalize the words of each element, and then select the movies that begin with the letter `d`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-repository",
   "metadata": {},
   "source": [
    "<img src=\"./filter_map.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-engine",
   "metadata": {},
   "source": [
    "But, imagine if we reversed the steps, so that we first selected the records that begin with `d`, and then capitalize all of the remaining elements.  This second way we are achieving the same results, but need to perform less work, as we only map through the selected records.  This is what Spark does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-injury",
   "metadata": {},
   "source": [
    "To do this, Spark allows us to chain methods, and determine how most efficiently to perform the method calls, only taking action when it needs to.  Let's see this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-membrane",
   "metadata": {},
   "source": [
    "### A little experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-shannon",
   "metadata": {},
   "source": [
    "If we run the code below, notice that nothing is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "found-magic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.map(lambda movie: movie.title())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-weekend",
   "metadata": {},
   "source": [
    "And even if we chain the map and the filter methods, still nothing is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "golden-receptor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.map(lambda movie: movie.title()).filter(lambda movie: movie[0] == 'd' or movie[0] == 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-cycle",
   "metadata": {},
   "source": [
    "It's only when we add a collect function on the end, will some data be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "integral-maine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Knight', 'Dunkirk']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.filter(lambda movie: movie[0] == 'd' or movie[0] == 'D').map(lambda movie: movie.title()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-acting",
   "metadata": {},
   "source": [
    "Nothing was returned when we ran the `map` and `collect` functions, because when we only executed those functions, Spark did not actually act on the data.  It only recorded the steps it needed to perform.  Then in the third line we finally did act on the data.  We told Spark that we want to both transform, and filter the data, and then return all of the results.  \n",
    "\n",
    "Spark then logged the transformations we requested, reordered the transformations to perform them in the most efficient way, and returned the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-north",
   "metadata": {},
   "source": [
    "> <img src=\"./filter_workflow.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-bhutan",
   "metadata": {},
   "source": [
    "> So above we can see that we start with the original RDD, and as we call `map` and `collect`, Spark simply logs that we'll need to perform the above steps.  It's only when we call `collect` that Spark then can determine the best way to perform the steps and then executes the steps accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-pricing",
   "metadata": {},
   "source": [
    "### Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-warner",
   "metadata": {},
   "source": [
    "So above we can see that the functions `map` and `filter` do not actually perform any work on our data.  Instead steps are only kicked off when we call the `collect` method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-aquatic",
   "metadata": {},
   "source": [
    "In Spark, the methods that kick off tasks and return results are called **actions** (eg. collect).  The methods like `map` and `transform` that do not are called `transformations`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-exemption",
   "metadata": {},
   "source": [
    "* Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-storage",
   "metadata": {},
   "source": [
    "So we already saw that transformations include `map` and `filter`, let's see a few more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-parts",
   "metadata": {},
   "source": [
    "> sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-explorer",
   "metadata": {},
   "source": [
    "Sample allows us to take a random sample from our dataset.  Notice that it does not return any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "coupled-classic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.sample(fraction = .2, withReplacement = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-juice",
   "metadata": {},
   "source": [
    "> Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mounted-halifax",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-assurance",
   "metadata": {},
   "source": [
    "So one way to think about transformations is that they have to look comb through our data, either to filter, or unique the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-mediterranean",
   "metadata": {},
   "source": [
    "* Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-refund",
   "metadata": {},
   "source": [
    "Actions are a bit more about the end result.  So far we've learned about `collect`, which returns all of the results of a series of transformations.  \n",
    "\n",
    "> Take\n",
    "\n",
    "If we want to limit our results to a subset, we can use the function `take`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "unable-spread",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark knight', 'dunkirk']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.distinct().take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-recruitment",
   "metadata": {},
   "source": [
    "The function `take` just limits the results to a specified number.  So here, `take` just returns the first two results.  It's similar to `LIMIT` in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-victory",
   "metadata": {},
   "source": [
    "> Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "empirical-spain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-mambo",
   "metadata": {},
   "source": [
    "Count simply counts the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-carry",
   "metadata": {},
   "source": [
    "So we can see that, our actions have a bit of finality to them.  To get a better sense of the transformation and action functions, it's worth looking at the [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-motorcycle",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-northern",
   "metadata": {},
   "source": [
    "[Berkley White Paper](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "\n",
    "[Pyspark RDD Methods blog](https://www.nbshare.io/notebook/403283317/How-To-Analyze-Data-Using-Pyspark-RDD/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
